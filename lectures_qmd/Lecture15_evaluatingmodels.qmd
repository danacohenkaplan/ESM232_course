---
title: "Lecture15_evaluatingmodels"
format: html
editor: visual
---

## Comparing Model with Observations

Why?

-   to make a "case" for why you selected this model for your project
-   to quantify uncertainty in your model outputs
-   to choose parameters that make the model perform well
    -   reduce parameter uncertainty

## Qantifying Model Performance {.scrollable}

-   Depends on type of model output

    -   single value (mean energy production from a solar panel)
    -   time series (streamflow, PM10, monthly energy production )
    -   spatial pattern (population, income level)
    -   space-time (pollution in different cites over time, forest biomass growth across different forests)

-   Depends on observations

    -   often not as "dense" as model output (e.g samples)
    -   sometimes you have a time series (e.g. streamflow, PM10)
    -   sometimes you have a spatial pattern (e.g. population, income level)
    -   often you are sampling (e.g mean PM10, mean streamflow) so there is also error/uncertainty in the observation

## Metrics

-   what is possible given available data
-   what is important to get "right" given model application

## Example (Picking model based on performance)

![Model Comparison](img/hydrometric.png)

[Staudinger, M., Stahl, K., Seibert, J., Clark, M. P., and Tallaksen, L. M.: Comparison of hydrological model structures based on recession and low flow simulations, Hydrol. Earth Syst. Sci. Discuss., 8, 6833-6866, doi:10.5194/hessd-8-6833-2011, 2011]{style="font-size:50%"} \## NSE (Nash-Sutcliffe Efficiency)

## Somme classic metrics

-   t.test (are the means different)
-   var.test (are the variances different)

Dynamic models (trends over space and time)

-   RMSE (Root Mean Square Error)
-   Correlation ($R^2$)
-   NSE (Nash-Sutcliffe Efficiency) (variance weighted correlation)

But you can code your own \* make them an R function for utility

## NSE

$NSE = \frac{\sum((m_i - o_i)^2)}{\sum((o_i - mean(o))^2}$

``` r
nse = function(m, o) {
  err = m - o
  meanobs = mean(o)
  mse = sum(err^2)
  ovar = sum((o - meanobs)^2)
  return(1 - mse / ovar)
}
```

------------------------------------------------------------------------

## Percent Error

Useful to see if over whole time series you have a bias

Error = $\frac{\bar{m} - \bar{o}}{\bar{o}} \times 100$

``` r
relerr = function(m, o) {
  err = m - o
  meanobs = mean(o)
  return(mean(err) / meanobs)
}
```

------------------------------------------------------------------------

## Soft Metrics: Fuzzy Evaluation

-   Handle uncertainty/imprecise data
-   Fuzzy membership functions useful in calibration based on performance

Many options - here's one example

$perf(x) = \begin{cases}
  0 & \text{if } x \leq a_1 \\
   \frac{x - a_1}{a_2 - a_1} & \text{if } a_1 \leq x < a_2 \\
  1 & \text{if } a_2 \leq x < a_3 \\
  \frac{a_4 - x}{a_4 - a_3} & \text{if } a_3 \leq x < a_4 \\
   0 & \text{if } x \geq a_4
\end{cases}$

[Seibert, J., and J. J. McDonnell, On the dialog between experimentalist and modeler in catchment hydrology: Use of soft data for multicriteria model calibration, Water Resour. Res., 38(11), 1241, doi:10.1029/2001WR000978, 2002]{style="font-size:50%"}

# Example application

------------------------------------------------------------------------

## Combining Metrics

-   Normalize metrics (0-1)
-   Make all increase with performance
-   Weighted sum or multiplicative approach

$Metric_A * Metric_B * Metric_C$

$Metric_A * weight_A + Metric_B * weight_B + Metric_C * weight_C$ where $weight_A + weight_B + weight_C = 1$

## Normalizing

SSE = $1/n*\sum((m_i - o_i)^2)$

How do I make this 0-1? and increasing values give better performance

## solution options

$L = (SSE)^{-n}$

$L = exp(-n * SSE)$

$L = (max(SSE) - SSE) / max(SSE)$

where $max(SSE)$ is the maximum value of SSE across all models or sometimes

$L = (SSE - min(SSE)) / (max(SSE) - min(SSE))$

------------------------------------------------------------------------

## Combined Performance

``` r
cper = function(m, o, weight.nse = 0.5, weight.relerr = 0.5) {
  nse = max(nse(m, o), 0)
  rel.err = relerr(m, o)
  merr = 1.0 - min(1.0, abs(rel.err) / max(abs(rel.err)))
  return(weight.nse * nse + weight.relerr * merr)
}
```

------------------------------------------------------------------------

## Calibration and Optimization

-   Generate parameter sets (e.g., LHS)
-   Compute metrics for each
-   Define acceptable thresholds
-   Use optimization or GLUE

------------------------------------------------------------------------

## Yield Model Example

``` r
compute_yield = function(T, P, irr, crop.pars) {
  with(as.list(crop.pars), {
    nyears = length(T)
    irr.peryear = rep(irr, nyears)
    water.input = P + irr.peryear
    yield = ifelse(water.input < max.water,
      tp * water.input - ts * abs(T - Topt) + base.yield,
      tp * max.water - ts * abs(T - Topt) + base.yield)
    return(pmax(yield, 0))
  })
}
```

------------------------------------------------------------------------

## Equifinality

-   Many parameter sets yield similar performance
-   Limits confidence in "best" calibration

------------------------------------------------------------------------

## GLUE Approach

-   Reject poor performers
-   Retain acceptable sets
-   Use ensemble to represent uncertainty

------------------------------------------------------------------------

## Model Averaging

-   Weighted average of outputs by performance
-   Produces MWE (Mean Weighted Estimate)

------------------------------------------------------------------------

## Summary

-   Choose models that are both **appropriate** and **good enough**
-   Validate using multiple methods
-   Quantify and communicate uncertainty
-   Consider equifinality in interpretation
